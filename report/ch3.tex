\label{ch:serialExec}
In this chapter we will focus on a new example from the Folly library: a Serial Executor \cite{serialExec}. This example makes use of two independant synchronization mechanisms, using two atomic locations. We will show how separation logic allows us to prove those two mechanisms almost independantly. We prove here some properties about this example using the EFC permission structure and FSL++. We have not proven it in the proof framework developped in \cite{pascal}, but we believe that this would be possible.

To the best of our knowledge, this is the first proof of this executor in FSL++,  as well as the only example making use of more than one synchronization mechanisms.

We now describe in more details this new example. The Folly Serial Executor allows a thread to delegate the execution of a task, by simply adding this task to the executor. Depending on its characteristics, the executor will then execute this task depending on its priority, or sequentially as is the serial executor case, or any other ordering. This is quite similar to what a thread scheduler does: it is given some tasks (the threads execution), and then choses when and how to run them.

In the case of the serial executor, the executor ensures that all the tasks that are added to it are executed sequentially. An example use case of such an executor would be in a server, for a port. We want to make sure that no two threads try to access the port at the same time. We can hence create a serial executor \texttt{s} dedicated to that port. Now, any thread requiring action on this port will add the task to this executor. The executor will then schedule those tasks one after the other, making sure not two tasks try to write to the port at the same time.

Note that it is possible that multiple threads execute the tasks assigned to the serial executor over time. The serial executor simply garantees that this will never be done concurrently: we will not have two threads executing some tasks from the serial executor at the same time.

Besides, a serial executor guarantees that even if it is deleted, the tasks that were added to it will be executed sequentially, as if it had not been deleted.\footnote{Unless the serial executor parent, that is the executor it delegates the tasks to is deleted.}



\section{Implementation}
A simplified implementation of the executor is shown in Figure~\ref{fig:simplSerialExec}. This code is quite different from the original, as it was originally divided amongst two classes, and used pointers, as well as customs destructors. A code more faithful to the original can be found in the Appendix~\ref{app:serialExec}. 

\begin{figure}
	\begin{lstlisting}
Executor newSerialExecutor(Executor parent){
  s = alloc();
  s.parent_ = parent;
  s.queue_ = new Queue();
  s.scheduled_ = 0;
  s.keepAliveCounter_ = 1;
  return s;
}

void add(Func func, Executor s){
	s.queue_.enqueue(func);
	s.parent_.add({this->run()});
}

void run(Executor s){
	if(s.scheduled_.fetch_add(1, acquire) > 0)
		return;
	do {
		Func func = s.queue_.dequeue();
		func();
	} while(s.scheduled_.fetch_sub(1, release) > 1);
}

void drop(Executor s){
	int c = s.keepAliveCounter_.fetch_sub(1, release_acquire);
	if(c == 1)
		free(s);
}

Executor copy(Executor s){
	s.keepAliveCounter_.fetch_add(1, relaxed);
	return s;
}
	\end{lstlisting}

		\caption{Simplified code for the Folly SerialExecutor}
\label{fig:simplSerialExec}
	%	\caption{Simplified code for the Folly SerialExecutor}
\end{figure}


An executor is made of four fields. The field \texttt{parent\_} refers to its parent executor, the one it will delegate the tasks to. Indeed, this serial executor is merely a middleman here. It records all the tasks that are submitted to it, then transforms them into one continuous task that it submits to its parent, another executor.\footnote{There is no infinite delegation here, at some point the executor's parent will be the CPU scheduler directly.} Note that we will hence be talking about two different kinds of tasks in the following dicsussion: the ones that were submitted to the serial executor, and the new ones that the serial executor created and submitted to its parent executor. The field \texttt{queue\_} is used to record the task to be executed. The queue that is used here is a concurrent single consumer multiple producer queue. The field \texttt{scheduled\_} is used to ensure sequential execution. Finally the field \texttt{keepAliveCounter\_} is used to track the number of references to this executor, and deleting it when there are no references to it anymore. The last two fileds are atomic locations.

\subsection{Ensuring sequential execution}
Let us now describe the functions of the serial executor interface. The \texttt{add} one is quite straightforward. It adds a new task, here denoted by \texttt{func} to the executor \texttt{s}. This is done by simply adding the task to the queue, and sending a new task to the parent executor: running this executor \texttt{run} function. 

The \texttt{run} function is the one ensuring serial execution of the tasks, using the \texttt{scheduled\_} location. To explain how it works, it helps to think about what could have been an alternative implementation of this function. We could have simply used \texttt{scheduled\_} as a flag recording wether or not there is currently a task executing. The simplified code would have been the following

\begin{lstlisting}
void run(Executor s){
	if(CAS_Acq(s.scheduled_, 0, 1)){
		while(!s.queue_.empty()){
			func = s.queue.pop();
			func()
		}
		CAS_Rel(s.scheduled_, 1, 0);
	}
}
\end{lstlisting}

We first try to set atomically \texttt{scheduled\_} to 1. If this succeeds, this is now the only \texttt{run()} task from the serial executor \texttt{s} that can be executed. Then as long as the queue is not empty, we keep executing one after the other all the tasks that were added to it. When none are left, the \texttt{run()} task releases the \texttt{scheduled\_} flag by setting it to 0. However, this function requires two calls to \texttt{queue\_} per loop execution. One checking if it is empty or not and the second popping an element\footnote{Note that here, there is no risk of having a non empty queue qhen the condition check is done, then its elements being removed by another thread before the \texttt{pop} is done. Indeed, as \texttt{scheduled\_} is set to one, only the current task can remove elements from the queue.}. The key remark here is that we can use scheduled to count the number of elements in the queue reliably. This is what is done in the \texttt{run} function shown in Figure~\ref{fig:simplSerialExec}.

Every time this function is called, it first increments \texttt{scheduled\_} by one. As \texttt{run} is called every time an element is added to the \texttt{queue\_} by the \texttt{add} function, \texttt{scheduled\_} records the number of elements in the queue. Now if \texttt{scheduled\_} was zero before the task incremented it, the task starts executing the tasks contained in \texttt{queue\_}. This is similar to the case where the \texttt{CAS\_Acq} on \texttt{scheduled\_} succeeded in the alternative implementation discussed above. Every time a task from \texttt{queue\_} is executed, \texttt{scheduled\_} is decreased by one. As \texttt{scheduled\_} tracks the number of elements in the queue, this ensures that we never try to pop from an empty queue\footnote{This is not trivial actually, it relies on the fact that no other thread can pop from the queue, and that due to the careful of ordering of actions on \texttt{scheduled\_} and \texttt{queue\_}, after the \texttt{fetch\_subb}, \texttt{scheduled\_} is smaller or equal to the number of elements in the queue.}. When \texttt{scheduled\_} reaches 0, that is to say when the value returned by \texttt{fetch\_sub} is 1, the task terminates. As in the alternative implementation shown above, \texttt{scheduled\_} has been set back to 0, allowing another \texttt{run()} task to start executing later.

We explained above the idea of the \texttt{run} function. Let us now develop on the precise synchronization mechanisms used. As we have seen, it is \texttt{scheduled\_} that allows execution to "pass" from one thread to another. For instance, imagine Thread 1 calls \texttt{run}. It does the first fetch and add, and sees that the former value was 0. It can hence start executing tasks. At some point, the fecth and sub brings \texttt{scheduled\_} back to 0. Thread 1 stops executing tasks. Then at some later point, Thread 2 calls \texttt{run}, and starts executing tasks. Here when Thread 2 takes over execution, we have to make sure that for any other observer thread, all the memory events done by Thread 1 as part of executing the tasks stay before all the memory actions Thread 2 will do. As Thread 1 used a fetch and sub release, we know that all events executed in Thread 1 before this fetch and sub will stay before the action that set \texttt{scheduled\_} to 0. Besides, as Thread 2 uses a fetch and add acquire, all events that are executed after this fetch and add cannot be re-ordered with it. We hence have the required synchronization:
\[
				\{\text{Tasks 1}\} 
				\xrightarrow{\text{release}} 
				\{\mathtt{sceduled\_} = 0\} 
				\xrightarrow{\text{reads}} 
				\{\mathtt{scheduled\_} = 1\} 
				\xrightarrow{\text{acquire}} 
				\{\text{Tasks 2}\}
		\]

where we use Tasks 1 (resp. 2) to refer to the tasks executed by Thread 1 (resp. 2).

\subsection{Ensuring proper deletion when deleting references}
The functions \texttt{drop} and \texttt{copy} ensure that when all references to a serial executor \texttt{s} are gone, it is deleted, but not before that. They do so in a fashion extremely similar to that of the Rust Atomic Reference Counter. \texttt{keepAliveCounter} tracks the number or references to the serial executor. Those references can be obtained through the \texttt{copy} function, and deleted using the \texttt{drop} function. The latter then deletes the serial executor if the reference dropped is the last one. 

The only difference between this code and the one of the Rust ARC is in the \texttt{drop} function. In the Rust ARC, the code is the following (taken from \cite{fsl})

\begin{lstlisting}
int c = s.keepAliveCounter_.fetch_sub(1. release);
if(c == 1){
	fence_acq;
	free(s);
}
\end{lstlisting}

For the serial executor, instead of using a fence acquire only for the last decrement, every fetch and sub contains an acquire synchronization. For all the decrements that do not yield 0, this extra synchronization does not affect the behavior we want to achieve. Finally, for the last decrement, having acquire included in the fetch and sub instead of a standalone fence does not change the synchronization provided by the function\footnote{In practice, there is a slight difference: a fence acquire prevents re-ordering of any read before it with any read or write after it, whereas a load acquire only forbid re-ordering between the load itself and any subsequent read or write.}.

For a more indepth explanation of the synchronization mechanisms at play here, refer to \cite{fsl}.

%The synchronization mechanism is quite simple. When acquiring a copy, we use an acquire synchronization, ensuring all events that happened

\section{Properties of the \texttt{SerialExecutor}}
We explained above the implementation of the serial executor. We will here detail the few properties we set out to prove about this code

The first property we want to show about this code, is that the \texttt{Serial\-Executor} is indeed serial, that is to say all functions passed to it through \texttt{add} are executed sequentially. As we are only interested in the synchronization mechanism here, we model this using the following simplification. We consider one non-atomic location \texttt{protected}, and consider that all fonctions passed to \texttt{add} need full ownership of this location for their execution, and nothing else. We hence have to show that when they are run, they are in a thread that does own this permission.

The second property we are interested in is a proper use of the queue: we need to make sure that at any point there is at most one consumer, while there can be many producers. To model this, we introduce two abstract predicates $\consQ$ and $\prodQ$. They are governed by the following properties:
\begin{equation}
		\frac{}{\{\mathsf{emp}\} \texttt{q = new Queue()} \{\mathsf{Consumer}(\texttt{q}) * \mathsf{Producer}(\texttt{q})\}} 
		\tag{\textsc{new queue}} 
		\label{eq:nQ}
\end{equation}
\begin{equation}
		\mathsf{Producer}(\texttt{q}) \iff \mathsf{Producer}(\texttt{q}) * \mathsf{Producer}(\texttt{q})
		\tag{\textsc{producer duplication}} 
		\label{eq:prodDup}
\end{equation}

The \eqref{eq:nQ} allows us to create a new queue, and creates permission for a consumer and a producer. The \eqref{eq:prodDup} equivalence allows us to duplicate the producer permission, allowing mutliple threads to add to the queue. These predicates should be thought of as some placeholders, that could be replaced with more precise predicates defining the behavior of a single consumer multiple producer queue.

Finally we want to prove that the \texttt{keepAliveCounter} works as expected, that is to say like the Rust Atomic Reference Counter. The specification is quite simple: the deallocation of the executor \texttt{s} is not done until all threads are done with using the fields of \texttt{s} that are deallocated. However, note that the specification\footnote{Read comments in the code} of the \texttt{SerialExecutor} requires that even if it is deleted, all the tasks submitted to it are will still be executed with the same garantees. Besides, in the original code the field \texttt{protected\_} of the \texttt{SerialExecutor} is not deleted upon deallocation of the executor. Thus we can rephrase this property about the \texttt{keepAliveCounter} more precisely as: no read to the \texttt{parent\_} and \texttt{queue\_} fields of the executor should race with the deallocation of this executor.

Some other properties could be proven on this code, for instance the fact that we never try to get an element from the empty queue. However, this would require first defining and proving the properties of a single consumer, multiple producer queue.

